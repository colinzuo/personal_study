{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/colinzuo/work/github/personal_study/tensorflow/book_caicloud/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/colinzuo/work/github/personal_study/tensorflow/book_caicloud/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/colinzuo/work/github/personal_study/tensorflow/book_caicloud/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/colinzuo/work/github/personal_study/tensorflow/book_caicloud/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 100 training steps, loss on training batch is 0.372965.\n",
      "After 200 training steps, loss on training batch is 0.449551.\n",
      "After 300 training steps, loss on training batch is 0.331021.\n",
      "After 400 training steps, loss on training batch is 0.346335.\n",
      "After 500 training steps, loss on training batch is 0.309875.\n",
      "After 600 training steps, loss on training batch is 0.192811.\n",
      "After 700 training steps, loss on training batch is 0.281512.\n",
      "After 800 training steps, loss on training batch is 0.209793.\n",
      "After 900 training steps, loss on training batch is 0.243703.\n",
      "After 1000 training steps, loss on training batch is 0.331859.\n",
      "After 1100 training steps, loss on training batch is 0.206486.\n",
      "After 1200 training steps, loss on training batch is 0.224199.\n",
      "After 1300 training steps, loss on training batch is 0.196237.\n",
      "After 1400 training steps, loss on training batch is 0.244271.\n",
      "After 1500 training steps, loss on training batch is 0.257423.\n",
      "After 1600 training steps, loss on training batch is 0.1796.\n",
      "After 1700 training steps, loss on training batch is 0.220153.\n",
      "After 1800 training steps, loss on training batch is 0.228619.\n",
      "After 1900 training steps, loss on training batch is 0.179156.\n",
      "After 2000 training steps, loss on training batch is 0.168702.\n",
      "After 2100 training steps, loss on training batch is 0.218119.\n",
      "After 2200 training steps, loss on training batch is 0.146449.\n",
      "After 2300 training steps, loss on training batch is 0.160409.\n",
      "After 2400 training steps, loss on training batch is 0.21219.\n",
      "After 2500 training steps, loss on training batch is 0.176591.\n",
      "After 2600 training steps, loss on training batch is 0.199503.\n",
      "After 2700 training steps, loss on training batch is 0.152971.\n",
      "After 2800 training steps, loss on training batch is 0.152582.\n",
      "After 2900 training steps, loss on training batch is 0.139997.\n",
      "After 3000 training steps, loss on training batch is 0.127696.\n",
      "After 3100 training steps, loss on training batch is 0.132063.\n",
      "After 3200 training steps, loss on training batch is 0.133689.\n",
      "After 3300 training steps, loss on training batch is 0.128185.\n",
      "After 3400 training steps, loss on training batch is 0.268454.\n",
      "After 3500 training steps, loss on training batch is 0.140422.\n",
      "After 3600 training steps, loss on training batch is 0.161216.\n",
      "After 3700 training steps, loss on training batch is 0.167588.\n",
      "After 3800 training steps, loss on training batch is 0.122322.\n",
      "After 3900 training steps, loss on training batch is 0.126728.\n",
      "After 4000 training steps, loss on training batch is 0.119811.\n",
      "After 4100 training steps, loss on training batch is 0.113701.\n",
      "After 4200 training steps, loss on training batch is 0.131483.\n",
      "After 4300 training steps, loss on training batch is 0.157374.\n",
      "After 4400 training steps, loss on training batch is 0.117623.\n",
      "After 4500 training steps, loss on training batch is 0.109186.\n",
      "After 4600 training steps, loss on training batch is 0.115051.\n",
      "After 4700 training steps, loss on training batch is 0.13176.\n",
      "After 4800 training steps, loss on training batch is 0.14228.\n",
      "After 4900 training steps, loss on training batch is 0.130727.\n",
      "After 5000 training steps, loss on training batch is 0.110936.\n",
      "After 5100 training steps, loss on training batch is 0.109058.\n",
      "After 5200 training steps, loss on training batch is 0.126641.\n",
      "After 5300 training steps, loss on training batch is 0.112633.\n",
      "After 5400 training steps, loss on training batch is 0.105669.\n",
      "After 5500 training steps, loss on training batch is 0.147274.\n",
      "After 5600 training steps, loss on training batch is 0.0996656.\n",
      "After 5700 training steps, loss on training batch is 0.0990951.\n",
      "After 5800 training steps, loss on training batch is 0.0982925.\n",
      "After 5900 training steps, loss on training batch is 0.0972636.\n",
      "After 6000 training steps, loss on training batch is 0.101773.\n",
      "After 6100 training steps, loss on training batch is 0.0979096.\n",
      "After 6200 training steps, loss on training batch is 0.116707.\n",
      "After 6300 training steps, loss on training batch is 0.0968061.\n",
      "After 6400 training steps, loss on training batch is 0.106977.\n",
      "After 6500 training steps, loss on training batch is 0.0970726.\n",
      "After 6600 training steps, loss on training batch is 0.0984737.\n",
      "After 6700 training steps, loss on training batch is 0.087678.\n",
      "After 6800 training steps, loss on training batch is 0.0891231.\n",
      "After 6900 training steps, loss on training batch is 0.0890334.\n",
      "After 7000 training steps, loss on training batch is 0.0860106.\n",
      "After 7100 training steps, loss on training batch is 0.0984785.\n",
      "After 7200 training steps, loss on training batch is 0.0879271.\n",
      "After 7300 training steps, loss on training batch is 0.0858585.\n",
      "After 7400 training steps, loss on training batch is 0.0811975.\n",
      "After 7500 training steps, loss on training batch is 0.0853245.\n",
      "After 7600 training steps, loss on training batch is 0.0861912.\n",
      "After 7700 training steps, loss on training batch is 0.0838165.\n",
      "After 7800 training steps, loss on training batch is 0.0813803.\n",
      "After 7900 training steps, loss on training batch is 0.0818673.\n",
      "After 8000 training steps, loss on training batch is 0.0835113.\n",
      "After 8100 training steps, loss on training batch is 0.0813852.\n",
      "After 8200 training steps, loss on training batch is 0.0788515.\n",
      "After 8300 training steps, loss on training batch is 0.0819495.\n",
      "After 8400 training steps, loss on training batch is 0.0854682.\n",
      "After 8500 training steps, loss on training batch is 0.0833026.\n",
      "After 8600 training steps, loss on training batch is 0.0882838.\n",
      "After 8700 training steps, loss on training batch is 0.0815134.\n",
      "After 8800 training steps, loss on training batch is 0.0740666.\n",
      "After 8900 training steps, loss on training batch is 0.073692.\n",
      "After 9000 training steps, loss on training batch is 0.0766832.\n",
      "After 9100 training steps, loss on training batch is 0.0745867.\n",
      "After 9200 training steps, loss on training batch is 0.0744202.\n",
      "After 9300 training steps, loss on training batch is 0.0715704.\n",
      "After 9400 training steps, loss on training batch is 0.0718748.\n",
      "After 9500 training steps, loss on training batch is 0.075637.\n",
      "After 9600 training steps, loss on training batch is 0.0745275.\n",
      "After 9700 training steps, loss on training batch is 0.0766784.\n",
      "After 9800 training steps, loss on training batch is 0.0778317.\n",
      "After 9900 training steps, loss on training batch is 0.0762144.\n",
      "After 10000 training steps, loss on training batch is 0.0678188.\n",
      "After 10100 training steps, loss on training batch is 0.0688853.\n",
      "After 10200 training steps, loss on training batch is 0.0663974.\n",
      "After 10300 training steps, loss on training batch is 0.0662913.\n",
      "After 10400 training steps, loss on training batch is 0.0758932.\n",
      "After 10500 training steps, loss on training batch is 0.0644531.\n",
      "After 10600 training steps, loss on training batch is 0.0662214.\n",
      "After 10700 training steps, loss on training batch is 0.0700465.\n",
      "After 10800 training steps, loss on training batch is 0.0641373.\n",
      "After 10900 training steps, loss on training batch is 0.0675932.\n",
      "After 11000 training steps, loss on training batch is 0.0727474.\n",
      "After 11100 training steps, loss on training batch is 0.0644073.\n",
      "After 11200 training steps, loss on training batch is 0.0680151.\n",
      "After 11300 training steps, loss on training batch is 0.0615776.\n",
      "After 11400 training steps, loss on training batch is 0.0636165.\n",
      "After 11500 training steps, loss on training batch is 0.0673254.\n",
      "After 11600 training steps, loss on training batch is 0.0595459.\n",
      "After 11700 training steps, loss on training batch is 0.0700436.\n",
      "After 11800 training steps, loss on training batch is 0.0729527.\n",
      "After 11900 training steps, loss on training batch is 0.0610194.\n",
      "After 12000 training steps, loss on training batch is 0.0615779.\n",
      "After 12100 training steps, loss on training batch is 0.0664501.\n",
      "After 12200 training steps, loss on training batch is 0.060185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 12300 training steps, loss on training batch is 0.0660718.\n",
      "After 12400 training steps, loss on training batch is 0.0678057.\n",
      "After 12500 training steps, loss on training batch is 0.0564256.\n",
      "After 12600 training steps, loss on training batch is 0.0564369.\n",
      "After 12700 training steps, loss on training batch is 0.0554723.\n",
      "After 12800 training steps, loss on training batch is 0.0542701.\n",
      "After 12900 training steps, loss on training batch is 0.0625511.\n",
      "After 13000 training steps, loss on training batch is 0.0569361.\n",
      "After 13100 training steps, loss on training batch is 0.0608923.\n",
      "After 13200 training steps, loss on training batch is 0.0559123.\n",
      "After 13300 training steps, loss on training batch is 0.0595475.\n",
      "After 13400 training steps, loss on training batch is 0.05229.\n",
      "After 13500 training steps, loss on training batch is 0.060093.\n",
      "After 13600 training steps, loss on training batch is 0.0518224.\n",
      "After 13700 training steps, loss on training batch is 0.0542938.\n",
      "After 13800 training steps, loss on training batch is 0.0506186.\n",
      "After 13900 training steps, loss on training batch is 0.0557114.\n",
      "After 14000 training steps, loss on training batch is 0.0547499.\n",
      "After 14100 training steps, loss on training batch is 0.0552569.\n",
      "After 14200 training steps, loss on training batch is 0.0569879.\n",
      "After 14300 training steps, loss on training batch is 0.0551576.\n",
      "After 14400 training steps, loss on training batch is 0.0536984.\n",
      "After 14500 training steps, loss on training batch is 0.0501237.\n",
      "After 14600 training steps, loss on training batch is 0.0539115.\n",
      "After 14700 training steps, loss on training batch is 0.0548678.\n",
      "After 14800 training steps, loss on training batch is 0.0530988.\n",
      "After 14900 training steps, loss on training batch is 0.0510502.\n",
      "After 15000 training steps, loss on training batch is 0.0518113.\n",
      "After 15100 training steps, loss on training batch is 0.0622532.\n",
      "After 15200 training steps, loss on training batch is 0.0477367.\n",
      "After 15300 training steps, loss on training batch is 0.0507536.\n",
      "After 15400 training steps, loss on training batch is 0.0478868.\n",
      "After 15500 training steps, loss on training batch is 0.0564438.\n",
      "After 15600 training steps, loss on training batch is 0.0527497.\n",
      "After 15700 training steps, loss on training batch is 0.0524914.\n",
      "After 15800 training steps, loss on training batch is 0.0516032.\n",
      "After 15900 training steps, loss on training batch is 0.0524179.\n",
      "After 16000 training steps, loss on training batch is 0.0487395.\n",
      "After 16100 training steps, loss on training batch is 0.0467096.\n",
      "After 16200 training steps, loss on training batch is 0.0488114.\n",
      "After 16300 training steps, loss on training batch is 0.0472502.\n",
      "After 16400 training steps, loss on training batch is 0.0605131.\n",
      "After 16500 training steps, loss on training batch is 0.0512782.\n",
      "After 16600 training steps, loss on training batch is 0.0495442.\n",
      "After 16700 training steps, loss on training batch is 0.0514284.\n",
      "After 16800 training steps, loss on training batch is 0.0431252.\n",
      "After 16900 training steps, loss on training batch is 0.0449512.\n",
      "After 17000 training steps, loss on training batch is 0.0472501.\n",
      "After 17100 training steps, loss on training batch is 0.0478487.\n",
      "After 17200 training steps, loss on training batch is 0.043872.\n",
      "After 17300 training steps, loss on training batch is 0.0457826.\n",
      "After 17400 training steps, loss on training batch is 0.0491328.\n",
      "After 17500 training steps, loss on training batch is 0.051512.\n",
      "After 17600 training steps, loss on training batch is 0.0501466.\n",
      "After 17700 training steps, loss on training batch is 0.0435139.\n",
      "After 17800 training steps, loss on training batch is 0.0465743.\n",
      "After 17900 training steps, loss on training batch is 0.0490106.\n",
      "After 18000 training steps, loss on training batch is 0.0505581.\n",
      "After 18100 training steps, loss on training batch is 0.0448171.\n",
      "After 18200 training steps, loss on training batch is 0.0442047.\n",
      "After 18300 training steps, loss on training batch is 0.0408147.\n",
      "After 18400 training steps, loss on training batch is 0.0452083.\n",
      "After 18500 training steps, loss on training batch is 0.0528364.\n",
      "After 18600 training steps, loss on training batch is 0.04023.\n",
      "After 18700 training steps, loss on training batch is 0.0462853.\n",
      "After 18800 training steps, loss on training batch is 0.0403886.\n",
      "After 18900 training steps, loss on training batch is 0.0466099.\n",
      "After 19000 training steps, loss on training batch is 0.0418923.\n",
      "After 19100 training steps, loss on training batch is 0.0425373.\n",
      "After 19200 training steps, loss on training batch is 0.0415788.\n",
      "After 19300 training steps, loss on training batch is 0.04512.\n",
      "After 19400 training steps, loss on training batch is 0.0401162.\n",
      "After 19500 training steps, loss on training batch is 0.0394106.\n",
      "After 19600 training steps, loss on training batch is 0.0492561.\n",
      "After 19700 training steps, loss on training batch is 0.0430098.\n",
      "After 19800 training steps, loss on training batch is 0.0538619.\n",
      "After 19900 training steps, loss on training batch is 0.0472332.\n",
      "After 20000 training steps, loss on training batch is 0.0410133.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "def get_weight_variable(shape, regularizer):\n",
    "    weights = tf.get_variable(\"weights\", shape,\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights\n",
    "\n",
    "def inference(input_tensor, regularizer):\n",
    "    with tf.variable_scope('layer1'):\n",
    "        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "        biases = tf.get_variable('biases', [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "        \n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "        biases = tf.get_variable('biases', [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "        \n",
    "    return layer2\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 20000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "MODEL_SAVE_PATH = 'model'\n",
    "MODEL_NAME = 'model_921.ckpt'\n",
    "\n",
    "def train(mnist):\n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference(x, regularizer)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    with tf.name_scope('moving_average'):\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_average_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.name_scope('loss_function'):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "        \n",
    "    with tf.name_scope('train_step'):\n",
    "        learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE,\n",
    "                                                  LEARNING_RATE_DECAY)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        \n",
    "    with tf.control_dependencies([train_step, variables_average_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    writer = tf.summary.FileWriter('log', tf.get_default_graph())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            \n",
    "            if i % 100 == 99:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys},\n",
    "                                              options=run_options, run_metadata=run_metadata)\n",
    "                print(\"After %d training steps, loss on training batch is %g.\" % (step, loss_value))\n",
    "                writer.add_run_metadata(run_metadata, 'step%03d' % step)\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "            else:\n",
    "                _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n",
    "                \n",
    "    writer.close()\n",
    "\n",
    "if True:\n",
    "    if os.path.exists(\"/Users\"):\n",
    "        mnist = input_data.read_data_sets(\"/Users/colinzuo/work/github/personal_study/\"\n",
    "                                          \"tensorflow/book_caicloud/mnist/\", one_hot=True)\n",
    "    else:\n",
    "        mnist = input_data.read_data_sets(\"D:\\\\work\\\\DataAnalysis\\\\study\\\\mnist\", one_hot=True)\n",
    "        \n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
